"use strict";(self.webpackChunkkinderheim=self.webpackChunkkinderheim||[]).push([[84218],{3905:function(e,t,a){a.d(t,{Zo:function(){return m},kt:function(){return u}});var r=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,i=function(e,t){if(null==e)return{};var a,r,i={},n=Object.keys(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=r.createContext({}),p=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},N=r.forwardRef((function(e,t){var a=e.components,i=e.mdxType,n=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),N=p(a),u=i,g=N["".concat(s,".").concat(u)]||N[u]||h[u]||n;return a?r.createElement(g,o(o({ref:t},m),{},{components:a})):r.createElement(g,o({ref:t},m))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var n=a.length,o=new Array(n);o[0]=N;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var p=2;p<n;p++)o[p]=a[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}N.displayName="MDXCreateElement"},78613:function(e,t,a){a.r(t),a.d(t,{assets:function(){return m},contentTitle:function(){return s},default:function(){return u},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return h}});var r=a(87462),i=a(63366),n=(a(67294),a(3905)),o=["components"],l={title:"Generative adversarial networks"},s="[Generative adversarial networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)",p={unversionedId:"machine-learning/neural-networks/generative-adversarial-networks",id:"machine-learning/neural-networks/generative-adversarial-networks",title:"Generative adversarial networks",description:"Links",source:"@site/docs/machine-learning/neural-networks/generative-adversarial-networks.md",sourceDirName:"machine-learning/neural-networks",slug:"/machine-learning/neural-networks/generative-adversarial-networks",permalink:"/kinderheim/machine-learning/neural-networks/generative-adversarial-networks",draft:!1,editUrl:"https://github.com/ecioran/kinderheim/docs/machine-learning/neural-networks/generative-adversarial-networks.md",tags:[],version:"current",frontMatter:{title:"Generative adversarial networks"},sidebar:"tutorialSidebar",previous:{title:"Neural networks",permalink:"/kinderheim/machine-learning/neural-networks/"},next:{title:"Graph neural networks",permalink:"/kinderheim/machine-learning/neural-networks/graph-neural-networks"}},m={},h=[{value:"Links",id:"links",level:2}],N={toc:h};function u(e){var t=e.components,a=(0,i.Z)(e,o);return(0,n.kt)("wrapper",(0,r.Z)({},N,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"generative-adversarial-networks"},(0,n.kt)("a",{parentName:"h1",href:"https://en.wikipedia.org/wiki/Generative_adversarial_network"},"Generative adversarial networks")),(0,n.kt)("h2",{id:"links"},"Links"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=9JpdAg6uMXs"},"Introduction to GANs, NIPS 2016 | Ian Goodfellow")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1406.2661"},"Generative Adversarial Networks (2014)")," - Framework for estimating generative models via an adversarial process. (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/goodfeli/adversarial"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/znxlwm/pytorch-generative-model-collections"},"PyTorch implementation of various GANs")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=nBcZGjxnpDY"},"So I Created A Fursona Generator... (2019)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVIDIA/pix2pixHD"},"pix2pixHD")," - PyTorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic image-to-image translation."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/unit8co/vegans"},"VeGANs")," - Library to easily train various existing GANs (Generative Adversarial Networks) in PyTorch."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/google/compare_gan"},"Compare GAN code")," - Offers TensorFlow implementations for many components related to Generative Adversarial Networks."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/huggingface/pytorch-pretrained-BigGAN"},"PyTorch pretrained BigGAN")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/stylegan2"},"StyleGAN2 in TensorFlow")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/rosinality/stylegan2-pytorch"},"StyleGAN 2 in PyTorch")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/tkarras/progressive_growing_of_gans"},"Progressive Growing of GANs for Improved Quality, Stability, and Variation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/sdv-dev/CTGAN"},"CTGAN")," - Implementation of our NeurIPS paper Modeling Tabular data using Conditional GAN."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/kozistr/Awesome-GANs"},"Awesome GANs with TensorFlow")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1906.02839"},"How to make a pizza: Learning a compositional layer-based GAN model (2019)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harskish/ganspace"},"GANSpace: Discovering Interpretable GAN Controls")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/firmai/tsgan"},"Time-series Generative Adversarial Networks")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/jerryli27/TwinGAN"},"TwinGAN")," - Unsupervised Image Translation for Human Portraits."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://keras.io/examples/generative/wgan_gp/"},"Wasserstein GAN (WGAN) with Gradient Penalty (GP)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://artbreeder.com/"},"Artbreeder")," - Extend Your Imagination with GANs. (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=23147392"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/JiahuiYu/generative_inpainting"},"Generative Image Inpainting")," - DeepFill v1/v2 with Contextual Attention and Gated Convolution."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/junyanz/BicycleGAN"},"BicycleGAN")," - PyTorch implementation for multimodal image-to-image translation."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/anvoynov/GANLatentDiscovery"},"Unsupervised Discovery of Interpretable Directions in the GAN Latent Space")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/switchablenorms/DeepFashion_Try_On"},"Towards Photo-Realistic Virtual Try-On by Adaptively Generating \u2194 Preserving Image Content")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/clovaai/tunit"},"Rethinking the Truly Unsupervised Image-to-Image Translation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/tg-bomze/StyleGAN2-Face-Modificator"},"StyleGAN2-Face-Modificator")," - Simple Encoder, Generator and Face Modificator with StyleGAN2."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/tg-bomze/Face-Depixelizer"},"Face-Depixelizer"),' - Based on "PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models".'),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/hojonathanho/diffusion"},"Denoising Diffusion Probabilistic Models")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/openai/imitation"},"Generative Adversarial Imitation Learning")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://hific.github.io/"},"High-Fidelity Generative Image Compression")," (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=23652753"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://medium.com/@voshart/photoreal-roman-emperor-project-236be7f06c8f"},"Photoreal Roman Emperor Project. 54 Machine-learning assisted portraits (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://voshart.com/ROMAN-EMPEROR-PROJECT"},"Using machine learning to recreate photorealistic portraits of Roman Emperors")," (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=24172603"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/TachibanaYoshino/AnimeGANv2"},"AnimeGANv2")," - Improved version of AnimeGAN."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/thunil/TecoGAN"},"TecoGAN")," - TEmporally COherent GAN for video super-resolution. (",(0,n.kt)("a",{parentName:"li",href:"https://www.youtube.com/watch?v=MwCgvYtOLS0"},"Video"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://thegradient.pub/playing-a-game-of-ganstruction/"},"Playing a game of GANstruction (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://ajolicoeur.wordpress.com/the-new-contender-to-gans-score-matching-with-langevin-sampling/"},"Score matching with Langevin Sampling: a new contender to GANs (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=24366524"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/gordicaleksa/pytorch-gans"},"PyTorch GANs")," - Contains PyTorch implementation of various GAN architectures."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2009.08454"},"ExGAN: Adversarial Generation of Extreme Samples (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/eladrich/pixel2style2pixel"},"Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/stylegan2-ada"},"StyleGAN2 with adaptive discriminator augmentation (ADA) in TensorFlow")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/giannisdaras/ylg"},"Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f"},"Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch) (2017)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/juntang-zhuang/Adabelief-Optimizer"},"AdaBelief Optimizer")," - NeurIPS 2020 Spotlight, trains fast as Adam, generalizes well as SGD, and is stable to train GANs."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html"},"From GAN to WGAN (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://genforce.github.io/idinvert/"},"In-Domain GAN Inversion for Real Image Editing")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/genforce/idinvert"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/davidbau/rewriting"},"Rewriting a Deep Generative Model")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/CSAILVision/gandissect"},"GANDissect")," - PyTorch-based tools for visualizing and understanding the neurons of a GAN."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://vole.wtf/ganksy/"},"GANksy")," - A.I. street artist."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.gamasutra.com/blogs/MaxSchulz/20201022/372349/GANSupported_Concept_Art_Workflows.php"},"GAN-Supported Concept Art Workflows (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://neptune.ai/blog/gan-loss-functions"},"Understanding GAN Loss Functions (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/godisboy/SN-GAN"},"PyTorch implementation of Spectral Normalization for Generative Adversarial Networks")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/weihaox/awesome-gan-inversion"},"Awesome GAN Inversion papers")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/abhinavsagar/gvae"},"Generate High Resolution Images With Generative Variational Autoencoder")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/lucidrains/stylegan2-pytorch"},"Simple StyleGan2 for PyTorch")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/hanzhanggit/StackGAN-v2"},"StackGAN-v2")," - PyTorch implementation."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/taki0112/Self-Attention-GAN-Tensorflow"},"Self-Attention-GAN TensorFlow")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/heykeetae/Self-Attention-GAN"},"Self-Attention GAN PyTorch")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN"},"StudioGAN")," - StudioGAN is a PyTorch library providing implementations of representative Generative Adversarial Networks (GANs) for conditional/unconditional image generation. (",(0,n.kt)("a",{parentName:"li",href:"https://www.reddit.com/r/MachineLearning/comments/qt10az/project_pytorch_implementations_of_37_gan_papers/"},"Reddit"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/lucidrains/lightweight-gan"},"Implementation of an extremely 'lightweight' GAN proposed in ICLR 2021, in PyTorch")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/uoguelph-mlrg/instance_selection_for_gans"},"Instance Selection for GANs")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/dvschultz/stylegan2-ada"},"StyleGAN2 with adaptive discriminator augmentation (ADA) in TensorFlow")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.pyimagesearch.com/2020/11/16/gans-with-keras-and-tensorflow/"},"GANs with Keras and TensorFlow (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/ShengxiLi/rcf_gan"},"RCF-GAN"),' - Implementation of RCF-GAN proposed in the paper "Reciprocal Adversarial Learning via Characteristic Functions".'),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html"},"Using GANs to Create Fantastical Creatures (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=25146610"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://thisxdoesnotexist.com/"},"This X Does Not Exist")," - Using generative adversarial networks (GAN), we can learn how to create realistic-looking fake versions of almost anything. (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=25176101"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/"},"Style-based GANs \u2013 Generating and Tuning Realistic Artificial Faces (2018)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/yandex-research/navigan"},"Navigating the GAN Parameter Space for Semantic Image Editing")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/mit-han-lab/data-efficient-gans"},"Data-Efficient GANs with DiffAugment")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2011.12026"},"Adversarial Generation of Continuous Images (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/universome/inr-gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/HideUnderBush/UI2I_via_StyleGAN2"},"Unsupervised image-to-image translation method via pre-trained StyleGAN2 network")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/JCBrouwer/maua-stylegan2"},"Experiments with StyleGAN2")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/justinpinkney/awesome-pretrained-stylegan2"},"Awesome Pretrained StyleGAN2")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2010.15040"},"Training Generative Adversarial Networks by Solving Ordinary Differential Equations (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/nshepperd/ode-gan-pytorch"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/zaidalyafeai/gan-mosaics"},"gan-mosaics")," - Models were trained using a Stylegan2-Ada model."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1803.01422"},"DAGs with no tears: Continuous Optimization for Structure Learning (2018)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/xunzheng/notears"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://rameenabdal.github.io/StyleFlow/"},"StyleFlow | StyleFlow: Attribute-conditioned Exploration of StyleGAN-generated Images using Conditional Continuous Normalizing Flows (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/RameenAbdal/StyleFlow"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/nperraud/gan_audio_inpainting"},"Audio inpainting with generative adversarial network")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1810.02851"},"Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks (2018)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/yaushian/Unparalleled-Text-Summarization-using-GAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/EvgenyKashin/stylegan2-distillation"},"StyleGAN2 Distillation for Feed-forward Image Manipulation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2001.06937"},"A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2010.02407"},"Adversarial Grammatical Error Correction (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2012.04781"},"You Only Need Adversarial Supervision for Semantic Image Synthesis (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/boschresearch/OASIS"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.05278"},"GAN Inversion: A Survey (2021)")," - GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model, for the image to be faithfully reconstructed from the inverted code by the generator."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/sefibk/KernelGAN"},"KernelGAN")," - Blind Super-Resolution Kernel Estimation using an Internal-GAN."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/kwotsin/mimicry"},"Mimicry")," - Lightweight PyTorch library aimed towards the reproducibility of GAN research."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1703.05921"},"Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery (2017)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/xtarx/Unsupervised-Anomaly-Detection-with-Generative-Adversarial-Networks"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/justinpinkney/toonify"},"Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/l4rz/scaling-up-stylegan2"},"Scaling up StyleGAN2")," - Achieving photorealistic quality by scaling up StyleGAN2."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/aydao/stylegan2-surgery"},"StyleGAN2-Surgery")," - StyleGAN2 fork with scripts and convenience modifications for creative media synthesis."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/l4rz/practical-aspects-of-stylegan2-training"},"Practical aspects of StyleGAN2 training")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1909.12224.pdf"},"Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/svip-lab/impersonator"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/giannisdaras/ilo"},"Intermediate Layer Optimization for Inverse Problems using Deep Generative Models")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2102.03141"},"CharacterGAN: Few-Shot Keypoint Character Animation and Reposing (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tohinz/CharacterGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2102.07074"},"TransGAN: Two Transformers Can Make One Strong GAN (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/VITA-Group/TransGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.amazon.science/blog/growing-generative-adversarial-networks-layer-by-layer"},"Growing generative adversarial networks, layer by layer (2021)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/dorarad/gansformer"},"GANsformer: Generative Adversarial Transformers")," (",(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2103.01209"},"Paper"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/mit-han-lab/anycost-gan"},"Anycost GANs for Interactive Image Synthesis and Editing")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arankomatsuzaki.wordpress.com/2021/03/04/state-of-the-art-image-generative-models/"},"State-of-the-Art Image Generative Models (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=26351754"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/Diyago/GAN-for-tabular-data"},"GANs for tabular data")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/AlexiaJM/AdversarialConsistentScoreMatching"},"Adversarial score matching and improved sampling for image generation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.02699"},"ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/yuval-alaluf/restyle-encoder"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/chrisdonahue/wavegan"},"WaveGAN")," - Learn to synthesize raw audio with generative adversarial networks."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.03963"},"InfinityGAN: Towards Infinite-Resolution Image Synthesis (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/hubert0527/infinityGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1806.00035"},"Assessing Generative Models via Precision and Recall (2018)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/msmsajjadi/precision-recall-distributions"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.06820"},"Few-shot Image Generation via Cross-domain Correspondence (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/utkarshojha/few-shot-gan-adaptation"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1912.01865"},"StarGAN v2: Diverse Image Synthesis for Multiple Domains (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/clovaai/stargan-v2"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/eps696/stargan2"},"StarGAN2 for practice")," - Version of StarGAN2 intended mostly for fellow artists."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/LdDl/gan-go"},"Generative Adversarial Network in Go via Gorgonia")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2003.12267"},"Controllable Person Image Synthesis with Attribute-Decomposed GAN (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/menyifang/ADGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://nv-tlabs.github.io/datasetGAN/"},"DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort (2021)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2004.07682"},"On the use of Benford's law to detect GAN-generated images (2020)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2102.04776"},"Generative Models as Distributions of Functions (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/EmilienDupont/neural-function-distributions"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/taki0112/GAN_Metrics-Tensorflow"},"GAN_Metrics-Tensorflow")," - Simple Tensorflow implementation of metrics for GAN evaluation."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1909.07083"},"Controllable Text-to-Image Generation (2019)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/mrlibw/ControlGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.microsoft.com/en-us/research/blog/how-can-generative-adversarial-networks-learn-real-life-distributions-easily/"},"How can generative adversarial networks learn real-life distributions easily (2021)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.05609"},"GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/rusty1s/pyg_autoscale"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.06561"},"GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/mchong6/GANsNRoses"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2006.11239"},"Denoising Diffusion Probabilistic Models (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/lucidrains/denoising-diffusion-pytorch"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.01505"},"Barbershop: GAN-based Image Compositing using Segmentation Masks (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/ZPdesu/Barbershop"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://nvlabs.github.io/alias-free-gan/"},"Alias-Free GAN")," (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=27606347"},"HN"),") (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/alias-free-gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2103.00430"},"Training Generative Adversarial Networks in One Stage (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/zju-vipa/OSGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/rinongal/StyleGAN-nada"},"Zero-Shot non-adversarial domain adaptation of pre-trained generators")," (",(0,n.kt)("a",{parentName:"li",href:"https://stylegan-nada.github.io/"},"Web"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://marcoamonteiro.github.io/pi-GAN-website/"},"pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/marcoamonteiro/pi-GAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2105.05233"},"Diffusion Models Beat GANs on Image Synthesis (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/openai/guided-diffusion"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://realless.glitch.me/"},"Realless")," - Ways To Use Generative Adversarial Networks In Art."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2107.11186"},"LARGE: Latent-Based Regression through GAN Semantics (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/YotamNitzan/LARGE"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/jmtomczak/intro_dgm"},"Introduction to Deep Generative Modeling: Examples")," (",(0,n.kt)("a",{parentName:"li",href:"https://jmtomczak.github.io/blog/10/10_ddgms_lvm_p2.html"},"Web"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/nightrome/really-awesome-gan"},"Awesome GANs")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/nashory/gans-awesome-applications"},"GAN Awesome Applications")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1611.02163"},"Unrolled Generative Adversarial Networks")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/poolio/unrolled_gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/Zeleni9/pytorch-wgan"},"Pytorch implementation of DCGAN, WGAN-CP, WGAN-GP")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/yangxy/GPEN"},"GAN Prior Embedded Network for Blind Face Restoration in the Wild")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2109.05070"},"Instance-Conditioned GAN (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/ic_gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/lukemelas/pytorch-pretrained-gans"},"PyTorch Pretrained GANs")," - StyleGAN2, BigGAN, BigBiGAN, SAGAN, SNGAN, SelfCondGAN, and more."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1609.04802"},"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (2016)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/twtygqyy/pytorch-SRResNet"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://nvlabs.github.io/stylegan3/"},"Alias-Free Generative Adversarial Networks (StyleGAN3)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/stylegan3"},"Code"),") (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=28833213"},"HN"),") (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/ouhenio/stylegan3-projector"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/un1tz3r0/stylegan3"},"StyleGAN3-Fun")," - SOTA GANs are hard to train and to explore, and StyleGAN2/ADA/3 are no different."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://iashin.ai/SpecVQGAN"},"Taming Visually Guided Sound Generation (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/v-iashin/SpecVQGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/skycrapers/TecoGAN-PyTorch"},"TecoGAN-PyTorch")," - PyTorch Reimplementation of TecoGAN: Temporally Coherent GAN for Video Super-Resolution."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/descriptinc/cargan"},"Chunked Autoregressive GAN (CARGAN)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.09788"},"CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/PeterouZh/CIPS-3D"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/PeterouZh/GAN-Inversion"},"GAN-Inversion Papers")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2006.14208"},"Searching towards Class-Aware Generators for Conditional Generative Adversarial Networks (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/PeterouZh/NAS_cGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2011.13074"},"Omni-GAN: On the Secrets of cGANs and Beyond (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/PeterouZh/Omni-GAN-PyTorch"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.artnome.com/news/2018/11/14/helena-sarin-why-bigger-isnt-always-better-with-gans-and-ai-art"},"Helena Sarin: Why Bigger Isn\u2019t Always Better With GANs And AI Art (2018)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/open-mmlab/mmgeneration"},"MMGeneration")," - Powerful toolkit for generative models, based on PyTorch and MMCV."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://sites.google.com/view/projected-gan/"},"Projected GANs Converge Faster (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/autonomousvision/projected_gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.15678"},"A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/XingangPan/ShadeGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2008.05865"},"DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tobran/DF-GAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/dorarad/ganformer"},"GANformer: Generative Adversarial Transformers")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/Biomatter-Designs/ProteinGAN"},"ProteinGAN")," - Generative network architecture that may be used to produce de-novo protein sequences."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/PaddlePaddle/PaddleGAN"},"PaddleGAN")," - High-performance implementation of classic and SOTA Generative Adversarial Networks."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://apchenstu.github.io/sofgan/"},"SofGAN: A Portrait Image Generator with Dynamic Styling (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/apchenstu/sofgan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://onion-liu.github.io/BlendGAN/"},"BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/onion-liu/BlendGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2002.00349"},"Adversarial Generation of Continuous Implicit Shape Representations (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/marian42/shapegan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2111.15666"},"HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/yuval-alaluf/hyperstyle"},"Code"),") (",(0,n.kt)("a",{parentName:"li",href:"https://yuval-alaluf.github.io/hyperstyle/"},"Web"),") (",(0,n.kt)("a",{parentName:"li",href:"https://news.ycombinator.com/item?id=29401544"},"HN"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/maua-maua-maua/audiovisual"},"Audiovisual interpolations with GANs")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.wpeebles.com/gangealing"},"GAN-Supervised Dense Visual Alignment (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/wpeebles/gangealing"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/Sxela/ArcaneGAN"},"ArcaneGAN")," (",(0,n.kt)("a",{parentName:"li",href:"https://huggingface.co/spaces/akhaliq/ArcaneGAN"},"Web"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://nvlabs.github.io/denoising-diffusion-gan/"},"Tackling the Generative Learning Trilemma with Denoising Diffusion GANs (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/denoising-diffusion-gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.cs.cmu.edu/~vision-aided-gan/"},"Ensembling Off-the-shelf Models for GAN Training (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/nupurkmr9/vision-aided-gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/mafda/generative_adversarial_networks_101"},"Generative Adversarial Networks 101")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2006.06676"},"StyleGAN2-ADA: Training Generative Adversarial Networks with Limited Data (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/stylegan2-ada-pytorch"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://matthew-a-chan.github.io/EG3D/"},"EG3D: Efficient Geometry-aware 3D GANs (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/NVlabs/eg3d"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/mchong6/JoJoGAN"},"JoJoGAN: One Shot Face Stylization")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://genforce.github.io/volumegan/"},"VolumeGAN - 3D-aware Image Synthesis via Learning Structural and Textural Representations (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/genforce/volumegan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://genforce.github.io/"},"GenForce: Research Initiative on Generative Modeling")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/genforce"},"GitHub"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://peterwang512.github.io/GANSketching/"},"Sketch Your Own GAN (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/PeterWang512/GANSketching"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/eps696/stylegan2ada"},"StyleGAN2-ada for practice")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/ouhenio/StyleGAN3-CLIP-notebooks"},"StyleGAN3 CLIP-based guidance")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://aiart.live/ca-gan/"},"CA-GAN: Composition-Aided GANs (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/fei-hdu/ca-gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://tamarott.github.io/SinGAN.htm"},"SinGAN: Learning a Generative Model from a Single Natural Image (2019)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/tamarott/SinGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://rameenabdal.github.io/Labels4Free/"},"Labels4Free: Unsupervised Segmentation using StyleGAN (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/RameenAbdal/Labels4Free"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2106.12423"},"Alias-Free Generative Adversarial Networks (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/rosinality/alias-free-gan-pytorch"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/sberbank-ai/sber-swap"},"SberSwap")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.13369"},"Explaining in Style: Training a GAN to explain a classifier in StyleSpace (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/google/explaining-in-style"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://sites.google.com/view/stylegan-xl/"},"Scaling StyleGAN to Large Diverse Datasets")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/autonomousvision/stylegan_xl"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2101.06813"},"Fast and accurate learned multiresolution dynamical downscaling for precipitation (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/lzhengchun/DSGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/IBM/MAX-Image-Resolution-Enhancer"},"Upscale an image by a factor of 4, while generating photo-realistic details")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/chail/gan-ensembling"},"GAN Ensembling")," - Invert and perturb GAN images for test-time ensembling."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://nv-tlabs.github.io/editGAN/"},"EditGAN: High-Precision Semantic Image Editing (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/nv-tlabs/editGAN_release"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2201.13433"},"Third Time's the Charm? Image and Video Editing with StyleGAN3 (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/yuval-alaluf/stylegan3-editing"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://openreview.net/forum?id=hcoswsDHNAW"},"Fast AdvProp (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/meijieru/fast_advprop"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2003.09764"},"Lifespan Age Transformation Synthesis (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/royorel/Lifespan_Age_Transformation_Synthesis"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/williamyang1991/DualStyleGAN"},"DualStyleGAN - Official PyTorch Implementation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://www.cs.cmu.edu/~clean-fid/"},"On Aliased Resizing and Surprising Subtleties in GAN Evaluation (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/GaParmar/clean-fid"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2203.11105"},"High-fidelity GAN Inversion with Padding Space (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/EzioBy/padinv"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://tengfei-wang.github.io/HFGI/"},"High-Fidelity GAN Inversion for Image Attribute Editing (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/Tengfei-Wang/HFGI"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/nkolkin13/NeuralNeighborStyleTransfer"},"Neural Neighbor Style Transfer")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2112.00374"},"CLIPstyler: Image Style Transfer with a Single Text Condition (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/cyclomon/CLIPstyler"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/Ha0Tang/AttentionGAN"},"AttentionGAN for Unpaired Image-to-Image Translation & Multi-Domain Image-to-Image Translation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1904.01310"},"DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis (2019)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/MinfengZhu/DM-GAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2203.04036"},"StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/FeiiYin/StyleHEAT"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/gwang-kim/DiffusionCLIP"},"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation (2022)")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2203.07932"},"Style Transformer for Image Inversion and Editing (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/sapphire497/style-transformer"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/paulbricman/thisrepositorydoesnotexist"},"This Repository Does Not Exist")," - Curated list of awesome projects which use Machine Learning to generate synthetic content."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://stitch-time.github.io/"},"Stitch it in Time: GAN-Based Facial Editing of Real Videos (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/rotemtzaban/STIT"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2204.04950"},"Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/FriedRonaldo/Primitives-PS"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://chail.github.io/anyres-gan/"},"Any-resolution Training for High-resolution Image Synthesis (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/chail/anyres-gan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2004.11660v2"},"Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning (2020)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/microsoft/DiscoFaceGAN"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://diff-ae.github.io/"},"Diffusion Autoencoders: Toward a Meaningful and Decodable Representation (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/phizaz/diffae"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/stylegan-human/StyleGAN-Human"},"StyleGAN-Human: A Data-Centric Odyssey of Human Generation")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://openreview.net/forum?id=Czsdv-S4-w9"},"Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks (2022)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/sihyun-yu/digan"},"Code"),")"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2104.00675"},"In&Out : Diverse Image Outpainting via GAN Inversion (2021)")," (",(0,n.kt)("a",{parentName:"li",href:"https://github.com/yccyenchicheng/InOut"},"Code"),")")))}u.isMDXComponent=!0}}]);